---
title: "Semantically enhanced attention map‐driven occluded person re‐identification"
collection: publications
permalink: /publication/2024-05-10-paper-Semantically enhanced attention map‐driven occluded person re‐identification-6
excerpt: ''
date: 2024-05-10
venue: 'Electronics Letters (JCR Q4)'
paperurl: 'https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=SBoHvVQAAAAJ&citation_for_view=SBoHvVQAAAAJ:Y0pCki6q_DkC'
citation: 'Ge Y, Yu M, Chen Z, et al. Semantically enhanced attention map‐driven occluded person re‐identification[J]. Electronics Letters, 2024, 60(9): e13217.'
---

Occluded person re-identification (Re-ID) is to identify a particular person when the person's body parts are occluded. However, challenges remain in enhancing effective information representation and suppressing background clutter when considering occlusion scenes. This paper proposes a novel attention map-driven network (AMD-Net) for occluded person Re-ID. In AMD-Net, human parsing labels are introduced to supervise the generation of partial attention maps, while a spatial-frequency interaction module is suggested to complement the higher-order semantic information from the frequency domain. Furthermore, a Taylor-inspired feature filter for mitigating background disturbance and extracting fine-grained features is proposed. Moreover, a part-soft triplet loss, which is robust to non-discriminative body partial features is also designed. Experimental results on Occluded-Duke, Occluded-Reid, Market-1501, and Duke-MTMC datasets show that this method outperforms existing state-of-the-art methods. The code is available at: https://github.com/ISCLab-Bistu/SA-ReID.
